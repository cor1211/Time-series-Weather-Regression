import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import os
import pyspark.sql.functions as F
from pyspark.sql import SparkSession
from pyspark.ml import PipelineModel
from pyspark.ml.feature import VectorAssembler

# --- Cáº¤U HÃŒNH TRANG ---
st.set_page_config(
   page_title="Weather Forecast Demo - Big Data Project",
   page_icon="ğŸŒ¤ï¸",
   layout="wide"
)

# --- 1. KHá»I Táº O SPARK & CACHE (Äá»ƒ khÃ´ng pháº£i load láº¡i nhiá»u láº§n) ---
import socketserver
import sys

@st.cache_resource
def get_spark_session():
   if not hasattr(socketserver, "UnixStreamServer"):
      socketserver.UnixStreamServer = socketserver.TCPServer
    
   # Trá» Ä‘Ãºng Ä‘áº¿n thÆ° má»¥c vá»«a cÃ i/táº£i
   os.environ['JAVA_HOME'] = r"D:\software\java11" # ÄÆ°á»ng dáº«n cÃ i Java
   os.environ['HADOOP_HOME'] = r"D:\software\spark" # ThÆ° má»¥c chá»©a thÆ° má»¥c bin
   os.environ['PATH'] = os.environ['PATH'] + r";D:\software\spark\bin" # ThÃªm Ä‘Æ°á»ng dáº«n hadoop bin vÃ o PATH

   # Chá»‰ Ä‘á»‹nh Python cho Spark
   os.environ['PYSPARK_PYTHON'] = sys.executable
   os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable
   
   try:
      spark = SparkSession.builder \
         .appName("WeatherProject") \
         .master("local[*]") \
         .config("spark.driver.host", "127.0.0.1") \
         .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
         .getOrCreate()
      print("Khá»Ÿi táº¡o Spark thÃ nh cÃ´ng!")
      return spark
   except Exception as e:
      print(f"Váº«n cÃ²n lá»—i: {e}")
   
   

@st.cache_resource
def load_models(model_dir="saved_models"):
   """
   Load toÃ n bá»™ 12 models (6 cho LR, 6 cho RF) vÃ o bá»™ nhá»› Ä‘á»‡m
   """
   models = {}
   targets = [
      "Target_MinTemp_D1", "Target_MaxTemp_D1", "Target_Rainfall_D1",
      "Target_MinTemp_D2", "Target_MaxTemp_D2", "Target_Rainfall_D2"
   ]
   
   # Kiá»ƒm tra Ä‘Æ°á»ng dáº«n
   if not os.path.exists(model_dir):
      return None, "KhÃ´ng tÃ¬m tháº¥y thÆ° má»¥c saved_models!"

   for algo in ["LR", "RF"]:
      for target in targets:
         path = f"{model_dir}/{algo}_{target}"
         try:
               # Load PipelineModel (bao gá»“m cáº£ Scaler/Assembler)
               models[f"{algo}_{target}"] = PipelineModel.load(path)
         except Exception as e:
               print(f"Warning: KhÃ´ng load Ä‘Æ°á»£c {path}")
   return models, targets

# @st.cache_data
def load_test_data(data_path="test_data.parquet"):
   """
   Load dá»¯ liá»‡u Test. Cache data chuyá»ƒn sang Pandas Ä‘á»ƒ UI cháº¡y nhanh.
   """
   spark = get_spark_session()
   if os.path.exists(data_path):
      # Äá»c Parquet (nhanh hÆ¡n CSV)
      df = spark.read.parquet(data_path)
   elif os.path.exists(data_path.replace("parquet", "csv")):
      # Fallback sang CSV náº¿u khÃ´ng cÃ³ parquet
      df = spark.read.option("header", "true").option("inferSchema", "true").csv(data_path.replace("parquet", "csv"))
   else:
      return None
      
   # Sáº¯p xáº¿p theo ngÃ y
   return df.orderBy("Date")

# --- 2. GIAO DIá»†N CHÃNH ---

def main():
   st.title("ğŸŒ¤ï¸ Há»‡ Thá»‘ng Dá»± BÃ¡o Thá»i Tiáº¿t - Demo Spark MLlib")
   st.markdown("---")

   # Load dá»¯ liá»‡u
   spark = get_spark_session()
   
   # --- SIDEBAR: Cáº¤U HÃŒNH ---
   st.sidebar.header("âš™ï¸ Cáº¥u hÃ¬nh dá»± bÃ¡o")
   
   # 1. Load Data
   # Náº¿u Ä‘ang cháº¡y trÃªn mÃ¡y local mÃ  chÆ°a lÆ°u file
   # Tá»‘t nháº¥t: lÆ°u test_df.write.parquet("test_data.parquet") rá»“i load láº¡i á»Ÿ Ä‘Ã¢y.
   test_spark_df = load_test_data("test_data.parquet") 
   
   if test_spark_df is None:
      st.error("âš ï¸ KhÃ´ng tÃ¬m tháº¥y file dá»¯ liá»‡u test! HÃ£y lÆ°u 'test_df' ra file 'test_data.parquet' hoáº·c '.csv'.")
      return

   # Chuyá»ƒn má»™t pháº§n nhá» sang Pandas Ä‘á»ƒ lÃ m danh sÃ¡ch chá»n ngÃ y cho nhanh
   # Chá»‰ láº¥y cá»™t Date vÃ  Index Ä‘á»ƒ táº¡o Dropdown
   date_options = test_spark_df.select("Date") \
                            .dropDuplicates(["Date"]) \
                            .orderBy("Date", ascending=False) \
                            .limit(50) \
                            .toPandas()
   date_options['Date'] = pd.to_datetime(date_options['Date'])
   
   selected_date = st.sidebar.selectbox(
      "Chá»n NgÃ y Dá»± BÃ¡o (NgÃ y T):",
      options=date_options['Date'],
      format_func=lambda x: x.strftime('%d/%m/%Y')
   )

   # 2. Chá»n Model
   model_type = st.sidebar.radio("Chá»n Thuáº­t ToÃ¡n:", ["Random Forest (RF)", "Linear Regression (LR)"])
   algo_prefix = "RF" if "Random" in model_type else "LR"

   # 3. Load Models
   models, targets = load_models()
   if models is None:
      st.error(targets) # In lá»—i Ä‘Æ°á»ng dáº«n
      return

   # --- MAIN COLUMN: Xá»¬ LÃ Dá»° BÃO ---
   
   # Láº¥y Ä‘Ãºng dÃ²ng dá»¯ liá»‡u cá»§a ngÃ y Ä‘Æ°á»£c chá»n
   # Filter trÃªn Spark DataFrame
   input_row_spark = test_spark_df.filter(F.col("Date") == selected_date)
   
   # Chuyá»ƒn sang Pandas Ä‘á»ƒ hiá»ƒn thá»‹ UI
   input_row_pdf = input_row_spark.toPandas()

   if input_row_pdf.empty:
      st.warning("KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u cho ngÃ y nÃ y.")
      return

   # --- PHáº¦N 1: THÃ”NG TIN Äáº¦U VÃ€O (CONTEXT) ---
   st.subheader(f"ğŸ“… ThÃ´ng tin Ä‘áº§u vÃ o: {selected_date.strftime('%d/%m/%Y')}")
   
   # Hiá»ƒn thá»‹ cÃ¡c chá»‰ sá»‘ quÃ¡ khá»© (Lag 1) Ä‘á»ƒ ngÆ°á»i xem náº¯m bá»‘i cáº£nh
   # Giáº£ sá»­ tÃªn cá»™t lÃ  MinTemp_Lag1, MaxTemp_Lag1... 
   cols = st.columns(3)
   try:
      cols[0].metric("MinTemp (HÃ´m qua)", f"{input_row_pdf['MinTemp_L1'].iloc[0]} Â°C")
      cols[1].metric("MaxTemp (HÃ´m qua)", f"{input_row_pdf['MaxTemp_L1'].iloc[0]} Â°C")
      cols[2].metric("Rainfall (HÃ´m qua)", f"{input_row_pdf['Rainfall_L1'].iloc[0]} mm")
   except KeyError:
      st.info("Hiá»ƒn thá»‹ cá»™t Lag: Kiá»ƒm tra láº¡i tÃªn cá»™t trong DataFrame (VD: MinTemp_Lag1 hay MinTemp_L1)")

   st.markdown("---")

   # --- PHáº¦N 2: THá»°C HIá»†N Dá»° BÃO ---
   st.subheader(f"ğŸš€ Káº¿t quáº£ Dá»± BÃ¡o ({model_type})")

   # Táº¡o 2 tab cho NgÃ y 1 vÃ  NgÃ y 2
   tab1, tab2 = st.tabs(["Dá»± BÃ¡o NgÃ y Mai (D1)", "Dá»± BÃ¡o NgÃ y Kia (D2)"])

   # HÃ m helper Ä‘á»ƒ láº¥y káº¿t quáº£
   def get_prediction(target_name, row_spark):
      model_key = f"{algo_prefix}_{target_name}"
      if model_key not in models:
         return 0.0, 0.0 # Model chÆ°a train hoáº·c lá»—i tÃªn
         
      # Predict
      pred_df = models[model_key].transform(row_spark)
      pred_val = pred_df.select("prediction").collect()[0][0]
      
      # Láº¥y giÃ¡ trá»‹ thá»±c táº¿ (Label) cÃ³ sáºµn trong test set Ä‘á»ƒ so sÃ¡nh
      actual_val = pred_df.select(target_name).collect()[0][0]
      
      return pred_val, actual_val

   # --- TAB 1: NGÃ€Y MAI (D1) ---
   with tab1:
      c1, c2, c3 = st.columns(3)
      
      # 1. MinTemp D1
      pred, actual = get_prediction("Target_MinTemp_D1", input_row_spark)
      delta = pred - actual
      c1.metric(label="Nhiá»‡t Ä‘á»™ Tháº¥p nháº¥t", value=f"{pred:.1f} Â°C", 
               delta=f"Lá»‡ch: {delta:.1f} Â°C", delta_color="inverse")
      c1.caption(f"Thá»±c táº¿: {actual} Â°C")

      # 2. MaxTemp D1
      pred, actual = get_prediction("Target_MaxTemp_D1", input_row_spark)
      delta = pred - actual
      c2.metric(label="Nhiá»‡t Ä‘á»™ Cao nháº¥t", value=f"{pred:.1f} Â°C", 
               delta=f"Lá»‡ch: {delta:.1f} Â°C", delta_color="inverse")
      c2.caption(f"Thá»±c táº¿: {actual} Â°C")

      # 3. Rainfall D1
      pred, actual = get_prediction("Target_Rainfall_D1", input_row_spark)
      delta = pred - actual
      c3.metric(label="LÆ°á»£ng MÆ°a", value=f"{pred:.1f} mm", 
               delta=f"Lá»‡ch: {delta:.1f} mm", delta_color="inverse")
      c3.caption(f"Thá»±c táº¿: {actual} mm")

      # Actionable Insight
      if pred > 5.0:
         st.warning("âš ï¸ Dá»± bÃ¡o cÃ³ mÆ°a Ä‘Ã¡ng ká»ƒ! NÃªn mang theo Ã´/Ã¡o mÆ°a.")
      elif pred > 0.5:
            st.info("â„¹ï¸ CÃ³ kháº£ nÄƒng mÆ°a nhá».")
      else:
         st.success("â˜€ï¸ Trá»i táº¡nh rÃ¡o.")

   # --- TAB 2: NGÃ€Y KIA (D2) ---
   with tab2:
      c1, c2, c3 = st.columns(3)
      
      # 1. MinTemp D2
      pred, actual = get_prediction("Target_MinTemp_D2", input_row_spark)
      delta = pred - actual
      c1.metric(label="Nhiá»‡t Ä‘á»™ Tháº¥p nháº¥t", value=f"{pred:.1f} Â°C", delta=f"{delta:.1f}", delta_color="inverse")

      # 2. MaxTemp D2
      pred, actual = get_prediction("Target_MaxTemp_D2", input_row_spark)
      delta = pred - actual
      c2.metric(label="Nhiá»‡t Ä‘á»™ Cao nháº¥t", value=f"{pred:.1f} Â°C", delta=f"{delta:.1f}", delta_color="inverse")

      # 3. Rainfall D2
      pred, actual = get_prediction("Target_Rainfall_D2", input_row_spark)
      delta = pred - actual
      c3.metric(label="LÆ°á»£ng MÆ°a", value=f"{pred:.1f} mm", delta=f"{delta:.1f}", delta_color="inverse")

   st.markdown("---")
   
   # --- PHáº¦N 3: BIá»‚U Äá»’ PHÃ‚N TÃCH ---
   
   # Láº¥y 100 ngÃ y xung quanh ngÃ y Ä‘Æ°á»£c chá»n Ä‘á»ƒ váº½ biá»ƒu Ä‘á»“
   # Logic: Filter ngÃ y > selected_date - 50 vÃ  ngÃ y < selected_date + 50
   # Äá»ƒ Ä‘Æ¡n giáº£n cho demo, ta váº½ 100 ngÃ y *sau* ngÃ y Ä‘Æ°á»£c chá»n
   
   st.subheader("ğŸ“ˆ PhÃ¢n tÃ­ch xu hÆ°á»›ng: Nhiá»‡t Ä‘á»™ & LÆ°á»£ng mÆ°a (30 ngÃ y tá»›i)")
   
   # 1. Chuáº©n bá»‹ dá»¯ liá»‡u váº½
   start_plot_date = F.date_sub(F.lit(selected_date), 30)
   end_plot_date = F.date_add(F.lit(selected_date), 30)
   
   chart_data = test_spark_df.filter(
                                 (F.col("Date") >= start_plot_date) & 
                                 (F.col("Date") <= end_plot_date)
                           ) \
                           .dropDuplicates(["Date"]) \
                           .orderBy("Date")
   # Láº¥y dá»¯ liá»‡u gá»‘c (Chá»©a Date vÃ  cÃ¡c cá»™t Target thá»±c táº¿)
   pdf_plot = chart_data.toPandas()

   if not pdf_plot.empty:
      # --- LOGIC Dá»° BÃO VÃ€ MERGE ---
      
      # 1. Xá»­ lÃ½ MinTemp
      key_min = f"{algo_prefix}_Target_MinTemp_D1"
      if key_min in models:
         # Predict trÃªn Spark
         res = models[key_min].transform(chart_data) \
                              .select("Date", "prediction") \
                              .withColumnRenamed("prediction", "Pred_Min")
         # Convert sang Pandas
         pdf_res = res.toPandas()
         # Merge vÃ o báº£ng chÃ­nh (Chá»‰ merge cá»™t Pred_Min)
         pdf_plot = pd.merge(pdf_plot, pdf_res, on="Date", how="left")
      else:
         # Náº¿u khÃ´ng cÃ³ model, Ä‘iá»n sá»‘ 0 Ä‘á»ƒ khÃ´ng lá»—i code váº½
         pdf_plot["Pred_Min"] = 0.0

      # 2. Xá»­ lÃ½ MaxTemp
      key_max = f"{algo_prefix}_Target_MaxTemp_D1"
      if key_max in models:
         res = models[key_max].transform(chart_data) \
                              .select("Date", "prediction") \
                              .withColumnRenamed("prediction", "Pred_Max")
         pdf_res = res.toPandas()
         pdf_plot = pd.merge(pdf_plot, pdf_res, on="Date", how="left")
      else:
         pdf_plot["Pred_Max"] = 0.0

      # 3. Xá»­ lÃ½ Rainfall
      key_rain = f"{algo_prefix}_Target_Rainfall_D1"
      if key_rain in models:
         res = models[key_rain].transform(chart_data) \
                                 .select("Date", "prediction") \
                                 .withColumnRenamed("prediction", "Pred_Rain")
         pdf_res = res.toPandas()
         pdf_plot = pd.merge(pdf_plot, pdf_res, on="Date", how="left")
      else:
         pdf_plot["Pred_Rain"] = 0.0
      
      # --- Váº¼ BIá»‚U Äá»’ (MATPLOTLIB) ---
      import matplotlib.dates as mdates
      
      # Táº¡o 2 biá»ƒu Ä‘á»“ con (Subplots)
      fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True)
      
      # --- BIá»‚U Äá»’ 1: NHIá»†T Äá»˜ ---
      # Kiá»ƒm tra cá»™t tá»“n táº¡i trÆ°á»›c khi váº½ Ä‘á»ƒ cháº¯c cháº¯n
      if 'Target_MinTemp_D1' in pdf_plot.columns:
         ax1.plot(pdf_plot['Date'], pdf_plot['Target_MinTemp_D1'], label="Min Thá»±c táº¿", color='blue', linestyle='-', alpha=0.5)
      ax1.plot(pdf_plot['Date'], pdf_plot['Pred_Min'], label="Min Dá»± bÃ¡o", color='navy', linestyle='--', linewidth=2)
      
      if 'Target_MaxTemp_D1' in pdf_plot.columns:
         ax1.plot(pdf_plot['Date'], pdf_plot['Target_MaxTemp_D1'], label="Max Thá»±c táº¿", color='orange', linestyle='-', alpha=0.5)
      ax1.plot(pdf_plot['Date'], pdf_plot['Pred_Max'], label="Max Dá»± bÃ¡o", color='red', linestyle='--', linewidth=2)
      
      ax1.set_title(f"PhÃ¢n tÃ­ch Bá»‘i cáº£nh (TrÆ°á»›c/Sau 30 ngÃ y) - {model_type}", fontweight='bold')
      
      # ThÃªm má»™t Ä‘Æ°á»ng káº» dá»c Ä‘á»ƒ Ä‘Ã¡nh dáº¥u ngÃ y hiá»‡n táº¡i (NgÃ y T)
      ax1.axvline(x=selected_date, color='black', linestyle='-', linewidth=1, label="NgÃ y Ä‘Æ°á»£c chá»n")
      ax2.axvline(x=selected_date, color='black', linestyle='-', linewidth=1)
      ax1.set_ylabel("Nhiá»‡t Ä‘á»™ (Â°C)")
      ax1.legend(loc="upper left")
      ax1.grid(True, linestyle=':', alpha=0.5)
      
      # --- BIá»‚U Äá»’ 2: LÆ¯á»¢NG MÆ¯A ---
      if 'Target_Rainfall_D1' in pdf_plot.columns:
         ax2.plot(pdf_plot['Date'], pdf_plot['Target_Rainfall_D1'], label="MÆ°a Thá»±c táº¿", color='#1f77b4', alpha=0.6)
      ax2.plot(pdf_plot['Date'], pdf_plot['Pred_Rain'], label="MÆ°a Dá»± bÃ¡o", color='green', linestyle='--', linewidth=2)
      
      ax2.set_title(f"Dá»± bÃ¡o LÆ°á»£ng mÆ°a - {model_type}", fontweight='bold')
      ax2.set_ylabel("LÆ°á»£ng mÆ°a (mm)")
      ax2.legend(loc="upper left")
      ax2.grid(True, linestyle=':', alpha=0.5)

      # Format ngÃ y thÃ¡ng
      ax2.xaxis.set_major_formatter(mdates.DateFormatter('%d/%m'))
      plt.xticks(rotation=45)
      
      st.pyplot(fig)
      
      # Báº£ng dá»¯ liá»‡u thÃ´ (Optional)
      with st.expander("Xem báº£ng sá»‘ liá»‡u chi tiáº¿t"):
         cols_to_show = ['Date', 'Pred_Min', 'Pred_Max', 'Pred_Rain']
         # Chá»‰ láº¥y cÃ¡c cá»™t Target náº¿u nÃ³ tá»“n táº¡i trong file test
         for t in ['Target_MinTemp_D1', 'Target_MaxTemp_D1', 'Target_Rainfall_D1']:
               if t in pdf_plot.columns:
                  cols_to_show.insert(1, t)
         st.dataframe(pdf_plot[cols_to_show])
         
   else:
      st.warning("KhÃ´ng Ä‘á»§ dá»¯ liá»‡u Ä‘á»ƒ váº½ biá»ƒu Ä‘á»“.")
if __name__ == "__main__":
   main()